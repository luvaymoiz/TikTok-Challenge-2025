{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bcbd34f",
   "metadata": {},
   "source": [
    "# Isolation Forest for Anomaly Detection\n",
    "\n",
    "In this notebook, we apply the **Isolation Forest (IF)** algorithm to detect spam reviews.  \n",
    "\n",
    "Isolation Forest is an unsupervised anomaly detection technique that works by **randomly partitioning data**.  \n",
    "- Anomalies are isolated faster because they differ significantly from the majority.  \n",
    "- The algorithm is particularly effective on high-dimensional datasets, making it suitable for text-based features.\n",
    "\n",
    "We will train the IF model on our processed dataset, extract anomaly scores, and label potential spam reviews.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7a76cd",
   "metadata": {},
   "source": [
    "## Step 1: Importing Libraries\n",
    "\n",
    "We first load the essential Python libraries for:  \n",
    "- **Data handling:** pandas, numpy  \n",
    "- **Preprocessing:** scikit-learn scalers  \n",
    "- **Modeling:** IsolationForest from scikit-learn  \n",
    "- **Exporting results:** saving outputs into CSV and Parquet formats  \n",
    "\n",
    "These libraries form the backbone of our anomaly detection workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6330af58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cce038",
   "metadata": {},
   "source": [
    "## Step 2: Loading Processed Dataset\n",
    "\n",
    "Next, we import the **cleaned and pre-processed dataset**.  \n",
    "This dataset contains the features engineered from the raw Google review data, such as:  \n",
    "- Review text embeddings / metadata features  \n",
    "- Reviewer and business-level statistics  \n",
    "\n",
    "These features will serve as input to the Isolation Forest model for anomaly detection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9914dedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('final_dataset.parquet')\n",
    "\n",
    "# Define the features to be used in the model\n",
    "features = [\n",
    "    'rating',\n",
    "    'text_len',\n",
    "    'rating_deviation',\n",
    "    'sentiment_polarity',\n",
    "    'sentiment_subjectivity',\n",
    "    'excessive_exclaim',\n",
    "    'avg_rating',\n",
    "    'log_num_reviews',\n",
    "    'price_encoded',\n",
    "    'year',\n",
    "    'month',\n",
    "    'weekday',\n",
    "    'hour',\n",
    "    'cat_American restaurant',\n",
    "    'cat_Coffee shop',\n",
    "    'cat_Department store',\n",
    "    'cat_Fast food restaurant',\n",
    "    'cat_Grocery store',\n",
    "    'cat_Hotel',\n",
    "    'cat_Mexican restaurant',\n",
    "    'cat_Other',\n",
    "    'cat_Pizza restaurant',\n",
    "    'cat_Restaurant',\n",
    "    'cat_Shopping mall'\n",
    "]\n",
    "\n",
    "# Check if all required features exist in the DataFrame\n",
    "missing_features = [f for f in features if f not in df.columns]\n",
    "if missing_features:\n",
    "    print(f\"Error: Missing required features in the dataset: {missing_features}\")\n",
    "    print(\"Please ensure your 'final-dataset.csv' contains these columns.\")\n",
    "    features = [f for f in features if f in df.columns]  # Proceed with available\n",
    "    if not features:\n",
    "        print(\"No valid features remaining. Exiting.\")\n",
    "        sys.exit(1)\n",
    "    else:\n",
    "        print(f\"Proceeding with available features: {features}\")\n",
    "\n",
    "# Use a temporary DataFrame for scaling to avoid modifying the original\n",
    "X = df[features].copy()\n",
    "\n",
    "# Handle potential NaNs in features (fill with mean)\n",
    "for col in features:\n",
    "    if X[col].isnull().any():\n",
    "        X[col] = X[col].fillna(X[col].mean())\n",
    "        print(f\"Filled NaN values in '{col}' with its mean.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e077e1af",
   "metadata": {},
   "source": [
    "## Step 3: Data Preprocessing\n",
    "\n",
    "Before training, we scale the features to ensure consistency.  \n",
    "- **Why scale?** Many ML algorithms, including IF, rely on distance-based measures.  \n",
    "  - Unscaled data may cause features with larger numerical ranges to dominate.  \n",
    "- We use `MinMaxScaler` to transform all features into the **[0,1] range**.  \n",
    "\n",
    "This ensures that every feature contributes equally to anomaly detection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5ec4c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cb64eb",
   "metadata": {},
   "source": [
    "## Step 4: Building the Isolation Forest Model\n",
    "\n",
    "We now initialize and train the Isolation Forest model:  \n",
    "- **n_estimators:** number of trees in the forest  \n",
    "- **contamination:** proportion of expected anomalies (controls thresholding)  \n",
    "- **random_state:** ensures reproducibility  \n",
    "\n",
    "The model assigns each review:  \n",
    "- An **anomaly score** (how “isolated” it is)  \n",
    "- An **anomaly label** (1 = normal, -1 = anomaly)\n",
    "\n",
    "These outputs help us flag suspicious or spam reviews.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb63ad2-087f-4563-b681-5db7f0758014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Isolation Forest model\n",
    "# 'contamination' is set to be 5% because we assume that 5% of the dataset is an anomaly\n",
    "model = IsolationForest(contamination=0.05, random_state=42)\n",
    "\n",
    "# Fit the model and get the predictions.\n",
    "# A prediction of -1 indicates an outlier, and 1 indicates an inlier.\n",
    "df['is_outlier'] = model.fit_predict(X_scaled)\n",
    "\n",
    "# Get the anomaly score. The lower the score, the more anomalous the point.\n",
    "df['anomaly_score'] = model.decision_function(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "845d2a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 26622 entries, 0 to 26621\n",
      "Data columns (total 38 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   user_id                   26622 non-null  object \n",
      " 1   name_review_user          26622 non-null  object \n",
      " 2   time                      26622 non-null  int64  \n",
      " 3   rating                    26622 non-null  int64  \n",
      " 4   text                      26622 non-null  object \n",
      " 5   gmap_id                   26622 non-null  object \n",
      " 6   latitude                  26622 non-null  float64\n",
      " 7   longitude                 26622 non-null  float64\n",
      " 8   category                  26616 non-null  object \n",
      " 9   avg_rating                26622 non-null  float64\n",
      " 10  num_of_reviews            26622 non-null  int64  \n",
      " 11  price                     14747 non-null  object \n",
      " 12  state                     15483 non-null  object \n",
      " 13  category_main             26622 non-null  object \n",
      " 14  cat_American restaurant   26622 non-null  bool   \n",
      " 15  cat_Coffee shop           26622 non-null  bool   \n",
      " 16  cat_Department store      26622 non-null  bool   \n",
      " 17  cat_Fast food restaurant  26622 non-null  bool   \n",
      " 18  cat_Grocery store         26622 non-null  bool   \n",
      " 19  cat_Hotel                 26622 non-null  bool   \n",
      " 20  cat_Mexican restaurant    26622 non-null  bool   \n",
      " 21  cat_Other                 26622 non-null  bool   \n",
      " 22  cat_Pizza restaurant      26622 non-null  bool   \n",
      " 23  cat_Restaurant            26622 non-null  bool   \n",
      " 24  cat_Shopping mall         26622 non-null  bool   \n",
      " 25  price_encoded             26622 non-null  int64  \n",
      " 26  year                      26622 non-null  int32  \n",
      " 27  month                     26622 non-null  int32  \n",
      " 28  weekday                   26622 non-null  int32  \n",
      " 29  hour                      26622 non-null  int32  \n",
      " 30  text_len                  26622 non-null  int64  \n",
      " 31  log_num_reviews           26622 non-null  float64\n",
      " 32  rating_deviation          26622 non-null  float64\n",
      " 33  sentiment_polarity        26622 non-null  float64\n",
      " 34  sentiment_subjectivity    26622 non-null  float64\n",
      " 35  excessive_exclaim         26622 non-null  bool   \n",
      " 36  is_outlier                26622 non-null  int64  \n",
      " 37  anomaly_score             26622 non-null  float64\n",
      "dtypes: bool(12), float64(8), int32(4), int64(6), object(8)\n",
      "memory usage: 5.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c48e44",
   "metadata": {},
   "source": [
    "## Step 5: Saving Results\n",
    "\n",
    "Finally, we export the model outputs (scores and anomaly flags) into multiple formats:  \n",
    "- **CSV:** widely used, easy for inspection and sharing  \n",
    "- **Parquet:** optimized for speed and storage efficiency, ideal for large datasets  \n",
    "\n",
    "This ensures the results can be easily re-used in downstream tasks, such as **fusion with Autoencoder scores**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5cffddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('IF_final_dataset_with_scores.csv', index=False)\n",
    "df.to_parquet('IF_final_dataset_with_scores.parquet', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
