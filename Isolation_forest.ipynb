{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adb63ad2-087f-4563-b681-5db7f0758014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "import sys\n",
    "\n",
    "# Load the dataset from the parquet file\n",
    "input_file = 'final_dataset.parquet'\n",
    "output_file = 'final_dataset_with_scores.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_parquet(input_file)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: '{input_file}' not found. Please ensure the file is in the same directory as the script.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# --- Feature Engineering ---\n",
    "\n",
    "# 1. Review length: A proxy for irrelevant content and rants.\n",
    "df['review_length'] = df['text'].astype(str).str.len()\n",
    "\n",
    "# 2. Number of reviews by user: A proxy for identifying non-visitors or new users.\n",
    "df['num_of_reviews_by_user'] = df.groupby('user_id')['text'].transform('count')\n",
    "\n",
    "# 3. Presence of ad keywords: A direct flag for advertisements.\n",
    "# The regex looks for common patterns like \"http://\", \"https://\", \"www.\", and a common phone number format.\n",
    "ad_regex = r'(http|www\\.|tel:|\\d{3}[-\\s]\\d{3}[-\\s]\\d{4})'\n",
    "df['has_ad_keywords'] = df['text'].astype(str).apply(\n",
    "    lambda x: 1 if re.search(ad_regex, x, re.I) else 0\n",
    ")\n",
    "\n",
    "# 4. Sentiment Polarity: Identifies extremely negative or positive reviews.\n",
    "df['sentiment_polarity'] = df['text'].astype(str).apply(\n",
    "    lambda x: TextBlob(x).sentiment.polarity\n",
    ")\n",
    "\n",
    "# 5. Star Rating: The original rating provided by the user (assuming it's in the data).\n",
    "# The EDA notebook suggests this exists in the original dataset.\n",
    "# We will use this column directly. We will fill any missing values just in case.\n",
    "df['star_rating'] = df['rating'].fillna(0)\n",
    "\n",
    "# Define the final features for the model\n",
    "features = [\n",
    "    'review_length',\n",
    "    'num_of_reviews_by_user',\n",
    "    'has_ad_keywords',\n",
    "    'sentiment_polarity',\n",
    "    'star_rating'\n",
    "]\n",
    "\n",
    "# Use a temporary DataFrame for scaling to avoid modifying the original\n",
    "X = df[features].copy()\n",
    "\n",
    "# --- Data Preprocessing ---\n",
    "\n",
    "# Scale the features. This is crucial for models that rely on distance, and good practice for Isolation Forest.\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# --- Model Building ---\n",
    "\n",
    "# Initialize the Isolation Forest model\n",
    "# 'contamination' is a hyperparameter that can be tuned.\n",
    "model = IsolationForest(contamination=0.05, random_state=42)\n",
    "\n",
    "# Fit the model and get the predictions.\n",
    "# A prediction of -1 indicates an outlier, and 1 indicates an inlier.\n",
    "df['is_outlier'] = model.fit_predict(X_scaled)\n",
    "\n",
    "# Get the anomaly score. The lower the score, the more anomalous the point.\n",
    "df['anomaly_score'] = model.decision_function(X_scaled)\n",
    "\n",
    "# --- Save the Output ---\n",
    "\n",
    "# Save the DataFrame with all the new features and scores to a new Parquet file.\n",
    "df.to_csv(output_file, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
