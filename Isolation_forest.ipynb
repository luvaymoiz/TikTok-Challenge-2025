{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adb63ad2-087f-4563-b681-5db7f0758014",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Repetition level histogram size mismatch",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m output_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_dataset_with_scores.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 12\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(input_file)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not found. Please ensure the file is in the same directory as the script.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parquet.py:667\u001b[0m, in \u001b[0;36mread_parquet\u001b[1;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[0;32m    664\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    665\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[1;32m--> 667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m impl\u001b[38;5;241m.\u001b[39mread(\n\u001b[0;32m    668\u001b[0m     path,\n\u001b[0;32m    669\u001b[0m     columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m    670\u001b[0m     filters\u001b[38;5;241m=\u001b[39mfilters,\n\u001b[0;32m    671\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    672\u001b[0m     use_nullable_dtypes\u001b[38;5;241m=\u001b[39muse_nullable_dtypes,\n\u001b[0;32m    673\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    674\u001b[0m     filesystem\u001b[38;5;241m=\u001b[39mfilesystem,\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    676\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parquet.py:274\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[1;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[0;32m    267\u001b[0m path_or_handle, handles, filesystem \u001b[38;5;241m=\u001b[39m _get_path_or_handle(\n\u001b[0;32m    268\u001b[0m     path,\n\u001b[0;32m    269\u001b[0m     filesystem,\n\u001b[0;32m    270\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    271\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    272\u001b[0m )\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 274\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[0;32m    275\u001b[0m         path_or_handle,\n\u001b[0;32m    276\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m    277\u001b[0m         filesystem\u001b[38;5;241m=\u001b[39mfilesystem,\n\u001b[0;32m    278\u001b[0m         filters\u001b[38;5;241m=\u001b[39mfilters,\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    280\u001b[0m     )\n\u001b[0;32m    281\u001b[0m     result \u001b[38;5;241m=\u001b[39m pa_table\u001b[38;5;241m.\u001b[39mto_pandas(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mto_pandas_kwargs)\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyarrow\\parquet\\core.py:1843\u001b[0m, in \u001b[0;36mread_table\u001b[1;34m(source, columns, use_threads, schema, use_pandas_metadata, read_dictionary, memory_map, buffer_size, partitioning, filesystem, filters, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification)\u001b[0m\n\u001b[0;32m   1831\u001b[0m     \u001b[38;5;66;03m# TODO test that source is not a directory or a list\u001b[39;00m\n\u001b[0;32m   1832\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m ParquetFile(\n\u001b[0;32m   1833\u001b[0m         source, read_dictionary\u001b[38;5;241m=\u001b[39mread_dictionary,\n\u001b[0;32m   1834\u001b[0m         memory_map\u001b[38;5;241m=\u001b[39mmemory_map, buffer_size\u001b[38;5;241m=\u001b[39mbuffer_size,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1840\u001b[0m         page_checksum_verification\u001b[38;5;241m=\u001b[39mpage_checksum_verification,\n\u001b[0;32m   1841\u001b[0m     )\n\u001b[1;32m-> 1843\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mread(columns\u001b[38;5;241m=\u001b[39mcolumns, use_threads\u001b[38;5;241m=\u001b[39muse_threads,\n\u001b[0;32m   1844\u001b[0m                     use_pandas_metadata\u001b[38;5;241m=\u001b[39muse_pandas_metadata)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyarrow\\parquet\\core.py:1485\u001b[0m, in \u001b[0;36mParquetDataset.read\u001b[1;34m(self, columns, use_threads, use_pandas_metadata)\u001b[0m\n\u001b[0;32m   1477\u001b[0m         index_columns \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   1478\u001b[0m             col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m _get_pandas_index_columns(metadata)\n\u001b[0;32m   1479\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, \u001b[38;5;28mdict\u001b[39m)\n\u001b[0;32m   1480\u001b[0m         ]\n\u001b[0;32m   1481\u001b[0m         columns \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1482\u001b[0m             \u001b[38;5;28mlist\u001b[39m(columns) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(index_columns) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(columns))\n\u001b[0;32m   1483\u001b[0m         )\n\u001b[1;32m-> 1485\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset\u001b[38;5;241m.\u001b[39mto_table(\n\u001b[0;32m   1486\u001b[0m     columns\u001b[38;5;241m=\u001b[39mcolumns, \u001b[38;5;28mfilter\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filter_expression,\n\u001b[0;32m   1487\u001b[0m     use_threads\u001b[38;5;241m=\u001b[39muse_threads\n\u001b[0;32m   1488\u001b[0m )\n\u001b[0;32m   1490\u001b[0m \u001b[38;5;66;03m# if use_pandas_metadata, restore the pandas metadata (which gets\u001b[39;00m\n\u001b[0;32m   1491\u001b[0m \u001b[38;5;66;03m# lost if doing a specific `columns` selection in to_table)\u001b[39;00m\n\u001b[0;32m   1492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_pandas_metadata:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyarrow\\_dataset.pyx:574\u001b[0m, in \u001b[0;36mpyarrow._dataset.Dataset.to_table\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyarrow\\_dataset.pyx:3865\u001b[0m, in \u001b[0;36mpyarrow._dataset.Scanner.to_table\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyarrow\\error.pxi:155\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyarrow\\error.pxi:92\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Repetition level histogram size mismatch"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "import sys\n",
    "\n",
    "# Load the dataset from the parquet file\n",
    "input_file = 'final_dataset.parquet'\n",
    "output_file = 'final_dataset_with_scores.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_parquet(input_file)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: '{input_file}' not found. Please ensure the file is in the same directory as the script.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# --- Feature Engineering ---\n",
    "\n",
    "# Use the same features as the Autoencoder model instead of the original simple features\n",
    "features = [\n",
    "    'rating',\n",
    "    'text_len',\n",
    "    'rating_deviation',\n",
    "    'sentiment_polarity',\n",
    "    'sentiment_subjectivity',\n",
    "    'excessive_exclaim',\n",
    "    'avg_rating',\n",
    "    'log_num_reviews',\n",
    "    'price_encoded',\n",
    "    'year',\n",
    "    'month',\n",
    "    'weekday',\n",
    "    'hour',\n",
    "    'cat_American restaurant',\n",
    "    'cat_Coffee shop',\n",
    "    'cat_Department store',\n",
    "    'cat_Fast food restaurant',\n",
    "    'cat_Grocery store',\n",
    "    'cat_Hotel',\n",
    "    'cat_Mexican restaurant',\n",
    "    'cat_Other',\n",
    "    'cat_Pizza restaurant',\n",
    "    'cat_Restaurant',\n",
    "    'cat_Shopping mall'\n",
    "]\n",
    "\n",
    "# Check if all required features exist in the DataFrame\n",
    "missing_features = [f for f in features if f not in df.columns]\n",
    "if missing_features:\n",
    "    print(f\"Error: Missing required features in the dataset: {missing_features}\")\n",
    "    print(\"Please ensure your 'final_dataset.parquet' contains these columns.\")\n",
    "    features = [f for f in features if f in df.columns]  # Proceed with available\n",
    "    if not features:\n",
    "        print(\"No valid features remaining. Exiting.\")\n",
    "        sys.exit(1)\n",
    "    else:\n",
    "        print(f\"Proceeding with available features: {features}\")\n",
    "\n",
    "# Use a temporary DataFrame for scaling to avoid modifying the original\n",
    "X = df[features].copy()\n",
    "\n",
    "# Handle potential NaNs in features (fill with mean)\n",
    "for col in features:\n",
    "    if X[col].isnull().any():\n",
    "        X[col] = X[col].fillna(X[col].mean())\n",
    "        print(f\"Filled NaN values in '{col}' with its mean.\")\n",
    "\n",
    "# --- Data Preprocessing ---\n",
    "\n",
    "# Scale the features. This is crucial for models that rely on distance, and good practice for Isolation Forest.\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# --- Model Building ---\n",
    "\n",
    "# Initialize the Isolation Forest model\n",
    "# 'contamination' is a hyperparameter that can be tuned.\n",
    "model = IsolationForest(contamination=0.05, random_state=42)\n",
    "\n",
    "# Fit the model and get the predictions.\n",
    "# A prediction of -1 indicates an outlier, and 1 indicates an inlier.\n",
    "df['is_outlier'] = model.fit_predict(X_scaled)\n",
    "\n",
    "# Get the anomaly score. The lower the score, the more anomalous the point.\n",
    "df['anomaly_score'] = model.decision_function(X_scaled)\n",
    "\n",
    "# --- Save the Output ---\n",
    "\n",
    "# Save the DataFrame with all the new features and scores to a new CSV file.\n",
    "df.to_csv(output_file, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
