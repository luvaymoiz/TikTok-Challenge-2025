{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffb3f7bc",
   "metadata": {},
   "source": [
    "# Fusion of Anomaly Scores (Isolation Forest + Autoencoder)\n",
    "\n",
    "In this step, we combine the outputs of **two different anomaly detection models**:  \n",
    "- **Isolation Forest (IF):** a tree-based method that isolates anomalies by splitting data.  \n",
    "- **Autoencoder (AE):** a neural-network model that reconstructs input data and flags large reconstruction errors as anomalies.  \n",
    "\n",
    "By fusing their results, we aim to leverage the strengths of both approaches.  \n",
    "This reduces reliance on a single method and increases robustness, since anomalies missed by one model might be caught by the other.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed59444",
   "metadata": {},
   "source": [
    "## Step 1: Importing Libraries and Datasets\n",
    "\n",
    "We begin by loading the necessary **Python libraries** for data handling, scaling, and anomaly detection.  \n",
    "We also import the **pre-computed results** from both the Isolation Forest and Autoencoder experiments.  \n",
    "\n",
    "These datasets contain anomaly scores for each review, which we will later align and combine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8769e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "AE_PATH  = \"AE_final_dataset_with_scores.parquet\"\n",
    "IF_PATH  = \"IF_final_dataset_with_scores.parquet\"\n",
    "BASE_IN  = \"final_dataset.parquet\"\n",
    "BASE_OUT_PARQUET = \"final_dataset_with_fusion.parquet\"\n",
    "BASE_OUT_CSV     = \"final_dataset_with_fusion.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9d01a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY_COL = None          # use this col to merge; else align by row order if 'None'\n",
    "AE_SCORE_COL = \"ae_reconstruction_error_zscore\" # score column name in AE file\n",
    "IF_SCORE_COL = \"anomaly_score\" # score column name in IF file\n",
    "\n",
    "NORMALIZE = True        # min-max per model before fusion\n",
    "FUSION    = \"mean\"      # fusion strategy: \"mean\" | \"max\" | \"rank_mean\"\n",
    "PERCENTILE = 95.0       # threshold = top 5% by default\n",
    "FIXED_THRESHOLD = None  # to override percentile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cffe1c9",
   "metadata": {},
   "source": [
    "### Normalization Functions\n",
    "\n",
    "- **`minmax_0_1`:** Scales values into the range **[0,1]**.  \n",
    "  - Useful when model scores are on different scales (e.g., one ranges from -1 to 1, another from 0 to 100).  \n",
    "  - Handles NaNs and constant columns gracefully to avoid errors.\n",
    "\n",
    "- **`to_rank_0_1`:** Converts scores into **percentile ranks [0,1]**.  \n",
    "  - This is useful if the raw score distributions between IF and AE are not directly comparable.  \n",
    "  - A score closer to 1 means the sample is more \"anomalous\" compared to others in the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21ec72b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax_0_1(s: pd.Series) -> pd.Series:\n",
    "    s = pd.to_numeric(s, errors=\"coerce\")\n",
    "    mn, mx = s.min(skipna=True), s.max(skipna=True)\n",
    "    if pd.isna(mn) or pd.isna(mx) or mn == mx:\n",
    "        return pd.Series(0.0, index=s.index)\n",
    "    return (s - mn) / (mx - mn)\n",
    "\n",
    "def to_rank_0_1(s: pd.Series) -> pd.Series:\n",
    "    return s.rank(method=\"average\", pct=True)  # uniform [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d385eea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "AE = pd.read_parquet(AE_PATH)\n",
    "IF = pd.read_parquet(IF_PATH)\n",
    "BASE = pd.read_parquet(BASE_IN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a261b0",
   "metadata": {},
   "source": [
    "Extracting columns and renaming for clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e8f55c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_df = AE[[KEY_COL, AE_SCORE_COL]].copy() if KEY_COL else AE[[AE_SCORE_COL]].copy()\n",
    "if KEY_COL: ae_df = ae_df.rename(columns={AE_SCORE_COL: \"ae_score\", KEY_COL: KEY_COL})\n",
    "else:       ae_df = ae_df.rename(columns={AE_SCORE_COL: \"ae_score\"})\n",
    "\n",
    "if_df = IF[[KEY_COL, IF_SCORE_COL]].copy() if KEY_COL else IF[[IF_SCORE_COL]].copy()\n",
    "if KEY_COL: if_df = if_df.rename(columns={IF_SCORE_COL: \"if_score\", KEY_COL: KEY_COL})\n",
    "else:       if_df = if_df.rename(columns={IF_SCORE_COL: \"if_score\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085404e2",
   "metadata": {},
   "source": [
    "### Step 2: Aligning Results\n",
    "\n",
    "We extract and rename the relevant columns from both the **Isolation Forest** and **Autoencoder** datasets.  \n",
    "This ensures consistency in column naming and allows us to merge them later without ambiguity.  \n",
    "\n",
    "The goal here is to create a unified table where each review has:  \n",
    "- Its IF anomaly score  \n",
    "- Its AE anomaly score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "622be835",
   "metadata": {},
   "outputs": [],
   "source": [
    "if KEY_COL:\n",
    "    fused = pd.merge(if_df, ae_df, on=KEY_COL, how=\"inner\", validate=\"one_to_one\")\n",
    "else:\n",
    "    # row-order alignment\n",
    "    n = min(len(ae_df), len(if_df))\n",
    "    fused = pd.concat([if_df.iloc[:n].reset_index(drop=True),\n",
    "                       ae_df.iloc[:n].reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82585d48",
   "metadata": {},
   "source": [
    "if NORMALIZE is toggled to yes, then normalise score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9217470",
   "metadata": {},
   "outputs": [],
   "source": [
    "if NORMALIZE:\n",
    "    s_if = minmax_0_1(fused[\"if_score\"])\n",
    "    s_ae = minmax_0_1(fused[\"ae_score\"])\n",
    "else:\n",
    "    s_if = pd.to_numeric(fused[\"if_score\"], errors=\"coerce\")\n",
    "    s_ae = pd.to_numeric(fused[\"ae_score\"], errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa303bd",
   "metadata": {},
   "source": [
    "### Step 3: Fusion Logic\n",
    "\n",
    "Now we apply **fusion strategies** to combine scores.  \n",
    "Different fusion modes may include:  \n",
    "- **Average:** Take the mean of IF and AE scores.  \n",
    "- **Max:** Take the higher score, emphasizing extreme anomalies.  \n",
    "- **Weighted:** Apply weights if we trust one model more than the other.  \n",
    "\n",
    "This step is crucial because it determines how anomalies from both models are consolidated into a **single anomaly score**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08405711",
   "metadata": {},
   "outputs": [],
   "source": [
    "if FUSION == \"mean\":\n",
    "    fusion = (s_if + s_ae) / 2.0\n",
    "elif FUSION == \"max\":\n",
    "    fusion = pd.concat([s_if, s_ae], axis=1).max(axis=1)\n",
    "elif FUSION == \"rank_mean\":\n",
    "    fusion = (to_rank_0_1(s_if) + to_rank_0_1(s_ae)) / 2.0\n",
    "else:\n",
    "    raise ValueError(f\"Unknown fusion method: {FUSION}\")\n",
    "\n",
    "fused[\"fusion_anomaly_score\"] = fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7eb9357",
   "metadata": {},
   "source": [
    "Pitting against threshold to determine the anomaly flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1969fe49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusion threshold: 0.478526\n",
      "Flagged anomalies: 1332/26622\n"
     ]
    }
   ],
   "source": [
    "if FIXED_THRESHOLD is not None:\n",
    "    th = float(FIXED_THRESHOLD)\n",
    "else:\n",
    "    th = float(np.percentile(fused[\"fusion_anomaly_score\"].dropna().values, PERCENTILE))\n",
    "\n",
    "fused[\"is_anomaly\"] = (fused[\"fusion_anomaly_score\"] >= th).astype(int)\n",
    "\n",
    "print(f\"Fusion threshold: {th:.6f}\")\n",
    "print(f\"Flagged anomalies: {int(fused['is_anomaly'].sum())}/{len(fused)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c740bd1",
   "metadata": {},
   "source": [
    "Merging back to base dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f874d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "if KEY_COL and KEY_COL in BASE.columns and KEY_COL in fused.columns:\n",
    "    out = pd.merge(\n",
    "        BASE,\n",
    "        fused[[KEY_COL, \"if_score\", \"ae_score\", \"fusion_anomaly_score\", \"is_anomaly\"]],\n",
    "        on=KEY_COL, how=\"left\", validate=\"one_to_one\"\n",
    "    )\n",
    "else:\n",
    "    m = min(len(BASE), len(fused))\n",
    "    out = BASE.iloc[:m].copy()\n",
    "    for c in [\"if_score\", \"ae_score\", \"fusion_anomaly_score\", \"is_anomaly\"]:\n",
    "        out[c] = fused.loc[:m-1, c].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca96b53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved:\n",
      "- Parquet: final_dataset_with_fusion.parquet\n",
      "- CSV    : final_dataset_with_fusion.csv\n"
     ]
    }
   ],
   "source": [
    "out.to_parquet(BASE_OUT_PARQUET, index=False)\n",
    "out.to_csv(BASE_OUT_CSV, index=False)\n",
    "print(\"Saved:\")\n",
    "print(f\"- Parquet: {BASE_OUT_PARQUET}\")\n",
    "print(f\"- CSV    : {BASE_OUT_CSV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca91c752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        0\n",
      "1        0\n",
      "2        0\n",
      "3        0\n",
      "4        0\n",
      "        ..\n",
      "26617    0\n",
      "26618    0\n",
      "26619    0\n",
      "26620    1\n",
      "26621    0\n",
      "Name: is_anomaly, Length: 26622, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(out[\"is_anomaly\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
